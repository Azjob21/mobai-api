{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "884dcec5",
   "metadata": {},
   "source": [
    "# MobAI'26 - Training Notebook\n",
    "## Task 1: Warehouse Optimization & Task 2: Demand Forecasting\n",
    "\n",
    "This notebook documents the complete training pipeline for both tasks:\n",
    "- **Task 2 (Forecasting)**: Prophet per-SKU + XGBoost Classifier + Blend\n",
    "- **Task 1 (Optimization)**: Heuristic-based warehouse optimization (no ML model needed)\n",
    "\n",
    "### Architecture Summary\n",
    "```\n",
    "Stage 0: Prophet per-SKU (cps=0.01, sps=0.1, multiplicative, 5 temporal regressors)\n",
    "Stage 1: XGBoost Classifier (demand yes/no, AUC=0.9375)\n",
    "Stage 2: Threshold + Blend + Bias tuning (WAPE=73.88%, Bias=0.62%)\n",
    "```\n",
    "\n",
    "### Key Results\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| WAPE (demand-days) | 73.88% |\n",
    "| Bias (demand-days) | 0.62% |\n",
    "| Classifier AUC | 0.9375 |\n",
    "| Improvement vs Lag-1 | 31.7% |\n",
    "| Improvement vs EWM-7 | 11.1% |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc10fcfc",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b54b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from pathlib import Path\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import json, warnings, time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "logging.getLogger(\"prophet\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"cmdstanpy\").setLevel(logging.WARNING)\n",
    "np.random.seed(42)\n",
    "\n",
    "BASE  = Path(\".\")  # Adjust if needed\n",
    "DATA  = BASE / \"data\"\n",
    "MODEL = BASE / \"models\"\n",
    "XLSX  = DATA / \"WMS_Hackathon_DataPack_Templates_FR_FV_B7_ONLY.xlsx\"\n",
    "\n",
    "print(f\"Data directory: {DATA.resolve()}\")\n",
    "print(f\"Model directory: {MODEL.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1bc79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data sources\n",
    "demand_raw = pd.read_excel(XLSX, \"historique_demande\")\n",
    "products   = pd.read_csv(DATA / \"product_priorities.csv\")\n",
    "segments   = pd.read_csv(DATA / \"product_segments.csv\")\n",
    "\n",
    "# Raw products table (physical attributes)\n",
    "products_raw = pd.read_excel(XLSX, \"produits\", header=0, skiprows=[1, 2])\n",
    "products_raw = products_raw[[\"id_produit\", \"categorie\", \"colisage fardeau\",\n",
    "                              \"colisage palette\", \"volume pcs (m3)\",\n",
    "                              \"Poids(kg)\", \"Is_Gerbable\"]].copy()\n",
    "products_raw.columns = [\"id_produit\", \"categorie_raw\", \"colisage_fardeau\",\n",
    "                         \"colisage_palette\", \"volume_pcs\", \"poids_kg\",\n",
    "                         \"is_gerbable\"]\n",
    "products_raw[\"is_gerbable\"] = products_raw[\"is_gerbable\"].map(\n",
    "    {True: 1.0, \"True\": 1.0, False: 0.0, \"False\": 0.0}\n",
    ").fillna(0.0)\n",
    "products_raw[\"volume_pcs\"]       = pd.to_numeric(products_raw[\"volume_pcs\"], errors=\"coerce\").fillna(0)\n",
    "products_raw[\"colisage_fardeau\"] = pd.to_numeric(products_raw[\"colisage_fardeau\"], errors=\"coerce\").fillna(1)\n",
    "products_raw[\"colisage_palette\"] = pd.to_numeric(products_raw[\"colisage_palette\"], errors=\"coerce\").fillna(1)\n",
    "products_raw[\"poids_kg\"]         = pd.to_numeric(products_raw[\"poids_kg\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Transactions (delivery patterns)\n",
    "transactions = pd.read_excel(XLSX, \"transactions\", header=0, skiprows=[1, 2])\n",
    "trans_lines  = pd.read_excel(XLSX, \"lignes_transaction\", header=0, skiprows=[1, 2])\n",
    "\n",
    "print(f\"Demand records: {len(demand_raw):,}\")\n",
    "print(f\"Products: {len(products)}\")\n",
    "print(f\"Segments: {segments['segment'].value_counts().to_dict()}\")\n",
    "print(f\"Transactions: {len(transactions):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a7c1d4",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63da309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore demand data\n",
    "demand_raw[\"date\"] = pd.to_datetime(demand_raw[\"date\"])\n",
    "demand_raw[\"day\"]  = demand_raw[\"date\"].dt.normalize()\n",
    "\n",
    "print(\"=== Demand Data ===\")\n",
    "print(f\"Date range: {demand_raw['day'].min().date()} to {demand_raw['day'].max().date()}\")\n",
    "print(f\"Unique products: {demand_raw['id_produit'].nunique()}\")\n",
    "print(f\"Total records: {len(demand_raw):,}\")\n",
    "print(f\"\\nDemand statistics:\")\n",
    "print(demand_raw['quantite_demande'].describe())\n",
    "\n",
    "# Daily aggregation\n",
    "daily = (\n",
    "    demand_raw.groupby([\"day\", \"id_produit\"])[\"quantite_demande\"]\n",
    "    .sum().reset_index()\n",
    "    .rename(columns={\"quantite_demande\": \"demand\"})\n",
    ")\n",
    "daily[\"demand\"] = daily[\"demand\"].clip(lower=0)\n",
    "\n",
    "# Outlier capping (99th percentile per SKU)\n",
    "cap = daily.groupby(\"id_produit\")[\"demand\"].quantile(0.99).rename(\"cap99\")\n",
    "daily = daily.merge(cap, on=\"id_produit\", how=\"left\")\n",
    "daily[\"demand\"] = daily[[\"demand\", \"cap99\"]].min(axis=1)\n",
    "daily.drop(columns=\"cap99\", inplace=True)\n",
    "\n",
    "all_prods = daily[\"id_produit\"].unique()\n",
    "all_dates = pd.date_range(daily[\"day\"].min(), daily[\"day\"].max(), freq=\"D\")\n",
    "print(f\"\\nAfter aggregation: {len(daily):,} product-day records\")\n",
    "print(f\"Products: {len(all_prods)}, Days: {len(all_dates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63e91cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsity analysis\n",
    "total_cells = len(all_prods) * len(all_dates)\n",
    "demand_cells = len(daily)\n",
    "sparsity = 1 - demand_cells / total_cells\n",
    "\n",
    "print(f\"Full grid size: {total_cells:,} cells\")\n",
    "print(f\"Cells with demand: {demand_cells:,}\")\n",
    "print(f\"Sparsity: {sparsity:.1%}\")\n",
    "\n",
    "# Demand frequency distribution\n",
    "prod_freq = daily.groupby(\"id_produit\")[\"day\"].nunique() / len(all_dates)\n",
    "print(f\"\\nDemand frequency distribution:\")\n",
    "print(prod_freq.describe())\n",
    "print(f\"\\nHigh frequency (>5% days): {(prod_freq > 0.05).sum()} products\")\n",
    "print(f\"Low frequency (<1% days): {(prod_freq < 0.01).sum()} products\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b122ff5c",
   "metadata": {},
   "source": [
    "## 3. Transaction-Derived Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fead721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build delivery pattern features from transactions\n",
    "transactions[\"cree_le\"] = pd.to_datetime(transactions[\"cree_le\"])\n",
    "deliveries = transactions[transactions[\"type_transaction\"] == \"DELIVERY\"].copy()\n",
    "deliveries[\"day\"] = deliveries[\"cree_le\"].dt.normalize()\n",
    "\n",
    "del_lines = trans_lines.merge(\n",
    "    deliveries[[\"id_transaction\", \"day\"]], on=\"id_transaction\", how=\"inner\"\n",
    ")\n",
    "\n",
    "daily_del = (\n",
    "    del_lines.groupby([\"day\", \"id_produit\"])\n",
    "    .agg(n_deliveries=(\"quantite\", \"count\"),\n",
    "         qty_delivered=(\"quantite\", \"sum\"))\n",
    "    .reset_index()\n",
    ")\n",
    "daily_del[\"qty_delivered\"] = daily_del[\"qty_delivered\"].clip(lower=0)\n",
    "\n",
    "prod_del_stats = (\n",
    "    daily_del.groupby(\"id_produit\")\n",
    "    .agg(\n",
    "        del_total_count=(\"n_deliveries\", \"sum\"),\n",
    "        del_total_qty=(\"qty_delivered\", \"sum\"),\n",
    "        del_n_days=(\"day\", \"nunique\"),\n",
    "        del_avg_qty=(\"qty_delivered\", \"mean\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"Delivery records: {len(daily_del):,}\")\n",
    "print(f\"Products with deliveries: {del_lines['id_produit'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165057c8",
   "metadata": {},
   "source": [
    "## 4. Full Calendar Grid Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa9330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build full grid (every product x every day)\n",
    "grid = pd.MultiIndex.from_product(\n",
    "    [all_dates, all_prods], names=[\"day\", \"id_produit\"]\n",
    ").to_frame(index=False)\n",
    "grid = grid.merge(daily, on=[\"day\", \"id_produit\"], how=\"left\")\n",
    "grid[\"demand\"] = grid[\"demand\"].fillna(0)\n",
    "grid[\"has_demand\"] = (grid[\"demand\"] > 0).astype(int)\n",
    "\n",
    "# Merge delivery counts\n",
    "grid = grid.merge(daily_del[[\"day\", \"id_produit\", \"n_deliveries\", \"qty_delivered\"]],\n",
    "                  on=[\"day\", \"id_produit\"], how=\"left\")\n",
    "grid[\"n_deliveries\"]  = grid[\"n_deliveries\"].fillna(0)\n",
    "grid[\"qty_delivered\"]  = grid[\"qty_delivered\"].fillna(0)\n",
    "\n",
    "# Merge segments and physical attributes\n",
    "grid = grid.merge(segments, on=\"id_produit\", how=\"left\")\n",
    "grid[\"segment\"] = grid[\"segment\"].fillna(\"LF\")\n",
    "grid[\"is_hf\"]   = (grid[\"segment\"] == \"HF\").astype(float)\n",
    "\n",
    "phys_cols = [\"id_produit\", \"colisage_fardeau\", \"colisage_palette\",\n",
    "             \"volume_pcs\", \"poids_kg\", \"is_gerbable\"]\n",
    "grid = grid.merge(products_raw[phys_cols], on=\"id_produit\", how=\"left\")\n",
    "for c in [\"colisage_fardeau\", \"colisage_palette\", \"volume_pcs\", \"poids_kg\", \"is_gerbable\"]:\n",
    "    grid[c] = grid[c].fillna(0)\n",
    "\n",
    "grid = grid.merge(prod_del_stats, on=\"id_produit\", how=\"left\")\n",
    "for c in [\"del_total_count\", \"del_total_qty\", \"del_n_days\", \"del_avg_qty\"]:\n",
    "    grid[c] = grid[c].fillna(0)\n",
    "\n",
    "grid.sort_values([\"id_produit\", \"day\"], inplace=True)\n",
    "grid.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Grid shape: {grid.shape}\")\n",
    "print(f\"Sparsity: {(grid['demand']==0).mean():.1%}\")\n",
    "print(f\"HF SKUs: {grid.loc[grid['is_hf']==1, 'id_produit'].nunique()}\")\n",
    "print(f\"LF SKUs: {grid.loc[grid['is_hf']==0, 'id_produit'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcb67aa",
   "metadata": {},
   "source": [
    "## 5. Stage 0: Prophet Per-SKU with Temporal Regressors\n",
    "\n",
    "### Model Configuration (Proven from Kaggle)\n",
    "- `changepoint_prior_scale = 0.01` (smooth trend)\n",
    "- `seasonality_prior_scale = 0.1` (moderate seasonality)\n",
    "- `seasonality_mode = \"multiplicative\"`\n",
    "- 5 temporal regressors: `day_of_week`, `is_weekend`, `month`, `is_month_start`, `is_month_end`\n",
    "- Per-product calibration factor (actual_mean / predicted_mean on demand-days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4ef3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet per-SKU fitting\n",
    "seg_map = segments.set_index(\"id_produit\")[\"segment\"].to_dict()\n",
    "grid_prophet = grid[[\"day\", \"id_produit\", \"demand\"]].copy()\n",
    "\n",
    "prod_demand = daily.groupby(\"id_produit\")[\"demand\"].agg([\"sum\", \"count\"]).reset_index()\n",
    "prod_demand.columns = [\"id_produit\", \"total_demand\", \"n_demand_days\"]\n",
    "\n",
    "MIN_DAYS_PROPHET = 10\n",
    "prophet_prods = set(prod_demand.loc[\n",
    "    prod_demand[\"n_demand_days\"] >= MIN_DAYS_PROPHET, \"id_produit\"\n",
    "])\n",
    "simple_prods = set(all_prods) - prophet_prods\n",
    "\n",
    "prod_freq = (\n",
    "    grid_prophet.groupby(\"id_produit\")\n",
    "    .apply(lambda g: (g[\"demand\"] > 0).mean())\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "print(f\"Products for Prophet fitting: {len(prophet_prods)}\")\n",
    "print(f\"Products for simple baseline: {len(simple_prods)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f914fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Prophet models (this takes ~10-15 minutes for all 571 products)\n",
    "prophet_predictions = {}\n",
    "prophet_models_meta = {}\n",
    "t0 = time.time()\n",
    "fitted_count = 0\n",
    "\n",
    "for i, prod_id in enumerate(list(prophet_prods)):\n",
    "    prod_data = grid_prophet[grid_prophet[\"id_produit\"] == prod_id][[\"day\", \"demand\"]].copy()\n",
    "    prod_data.columns = [\"ds\", \"y\"]\n",
    "    prod_data = prod_data.sort_values(\"ds\").reset_index(drop=True)\n",
    "    \n",
    "    seg = seg_map.get(prod_id, \"LF\")\n",
    "    freq = prod_freq.get(prod_id, 0.1)\n",
    "    \n",
    "    try:\n",
    "        m = Prophet(\n",
    "            growth=\"linear\",\n",
    "            yearly_seasonality=True,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=False,\n",
    "            seasonality_mode=\"multiplicative\",\n",
    "            changepoint_prior_scale=0.01,\n",
    "            seasonality_prior_scale=0.1,\n",
    "            interval_width=0.95,\n",
    "        )\n",
    "        \n",
    "        for reg_name in [\"day_of_week\", \"is_weekend\", \"month\",\n",
    "                         \"is_month_start\", \"is_month_end\"]:\n",
    "            m.add_regressor(reg_name, standardize=True)\n",
    "        \n",
    "        prod_data[\"day_of_week\"] = prod_data[\"ds\"].dt.dayofweek.astype(float)\n",
    "        prod_data[\"is_weekend\"]  = (prod_data[\"day_of_week\"] >= 5).astype(float)\n",
    "        prod_data[\"month\"]       = prod_data[\"ds\"].dt.month.astype(float)\n",
    "        prod_data[\"is_month_start\"] = prod_data[\"ds\"].dt.is_month_start.astype(float)\n",
    "        prod_data[\"is_month_end\"]   = prod_data[\"ds\"].dt.is_month_end.astype(float)\n",
    "        \n",
    "        m.fit(prod_data)\n",
    "        forecast = m.predict(prod_data[[\"ds\", \"day_of_week\", \"is_weekend\",\n",
    "                                        \"month\", \"is_month_start\", \"is_month_end\"]])\n",
    "        \n",
    "        yhat_raw = forecast[\"yhat\"].clip(lower=0).values\n",
    "        trend = forecast[\"trend\"].values\n",
    "        weekly = forecast.get(\"weekly\", pd.Series(0, index=forecast.index)).values\n",
    "        yearly = forecast.get(\"yearly\", pd.Series(0, index=forecast.index)).values\n",
    "        \n",
    "        # Per-product calibration\n",
    "        demand_mask = prod_data[\"y\"].values > 0\n",
    "        if demand_mask.sum() > 0:\n",
    "            actual_dd_mean = prod_data.loc[demand_mask, \"y\"].mean()\n",
    "            pred_dd_mean = yhat_raw[demand_mask].mean()\n",
    "            cal = actual_dd_mean / pred_dd_mean if pred_dd_mean > 0.01 else 1.0\n",
    "            cal = np.clip(cal, 0.2, 5.0)\n",
    "        else:\n",
    "            cal = 1.0\n",
    "        \n",
    "        yhat = yhat_raw * cal\n",
    "        \n",
    "        for j, (d, _) in enumerate(zip(prod_data[\"ds\"].values, yhat)):\n",
    "            prophet_predictions[(d, prod_id)] = {\n",
    "                \"yhat\": float(yhat[j]),\n",
    "                \"trend\": float(trend[j]),\n",
    "                \"weekly\": float(weekly[j]),\n",
    "                \"yearly\": float(yearly[j]),\n",
    "            }\n",
    "        \n",
    "        prophet_models_meta[int(prod_id)] = {\n",
    "            \"segment\": seg,\n",
    "            \"mean_yhat\": float(np.mean(yhat)),\n",
    "            \"trend_slope\": float((trend[-1] - trend[0]) / max(len(trend), 1)),\n",
    "            \"cal_factor\": float(cal),\n",
    "            \"demand_freq\": float(freq),\n",
    "        }\n",
    "        fitted_count += 1\n",
    "    except Exception:\n",
    "        simple_prods.add(prod_id)\n",
    "        prophet_prods.discard(prod_id)\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"   {i+1}/{len(prophet_prods)} fitted ({elapsed:.0f}s)\")\n",
    "\n",
    "print(f\"\\nFitted {fitted_count} Prophet models in {time.time()-t0:.0f}s\")\n",
    "\n",
    "# Simple baseline for remaining products\n",
    "simple_avg = {}\n",
    "for prod_id in simple_prods:\n",
    "    pdf = grid_prophet.loc[grid_prophet[\"id_produit\"] == prod_id, \"demand\"]\n",
    "    dd = pdf[pdf > 0]\n",
    "    simple_avg[prod_id] = max(float(dd.mean()), 0) if len(dd) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebd7489",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering (131 Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0beeb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Prophet predictions to grid\n",
    "grid[\"prophet_yhat\"] = 0.0\n",
    "grid[\"prophet_trend\"] = 0.0\n",
    "grid[\"prophet_weekly\"] = 0.0\n",
    "grid[\"prophet_yearly\"] = 0.0\n",
    "\n",
    "lookup_records = []\n",
    "for (day_val, prod_id), vals in prophet_predictions.items():\n",
    "    lookup_records.append({\n",
    "        \"day\": day_val, \"id_produit\": prod_id,\n",
    "        \"_pyhat\": vals[\"yhat\"], \"_ptrend\": vals[\"trend\"],\n",
    "        \"_pweekly\": vals[\"weekly\"], \"_pyearly\": vals[\"yearly\"],\n",
    "    })\n",
    "\n",
    "if lookup_records:\n",
    "    lookup_df = pd.DataFrame(lookup_records)\n",
    "    lookup_df[\"day\"] = pd.to_datetime(lookup_df[\"day\"])\n",
    "    grid = grid.merge(lookup_df, on=[\"day\", \"id_produit\"], how=\"left\")\n",
    "    for orig, tmp in [(\"prophet_yhat\", \"_pyhat\"), (\"prophet_trend\", \"_ptrend\"),\n",
    "                       (\"prophet_weekly\", \"_pweekly\"), (\"prophet_yearly\", \"_pyearly\")]:\n",
    "        grid[orig] = grid[tmp].fillna(grid[orig])\n",
    "        grid.drop(columns=tmp, inplace=True)\n",
    "\n",
    "for prod_id, avg in simple_avg.items():\n",
    "    mask = grid[\"id_produit\"] == prod_id\n",
    "    grid.loc[mask, \"prophet_yhat\"] = avg\n",
    "\n",
    "print(f\"Prophet yhat: mean={grid['prophet_yhat'].mean():.1f}, \"\n",
    "      f\"median={grid['prophet_yhat'].median():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798551d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-product demand statistics\n",
    "prod_stats = daily.groupby(\"id_produit\").agg(\n",
    "    prod_avg_demand=(\"demand\", \"mean\"),\n",
    "    prod_med_demand=(\"demand\", \"median\"),\n",
    "    prod_std_demand=(\"demand\", \"std\"),\n",
    "    prod_n_days=(\"day\", \"nunique\"),\n",
    ").reset_index()\n",
    "prod_stats[\"prod_std_demand\"] = prod_stats[\"prod_std_demand\"].fillna(1)\n",
    "grid = grid.merge(prod_stats, on=\"id_produit\", how=\"left\")\n",
    "\n",
    "# Lag features\n",
    "g = grid.groupby(\"id_produit\")[\"demand\"]\n",
    "for lag in [1, 2, 3, 7, 14, 21, 28]:\n",
    "    grid[f\"lag_{lag}\"] = g.shift(lag)\n",
    "\n",
    "# Rolling statistics\n",
    "shifted = g.shift(1)\n",
    "for w in [3, 7, 14, 28, 60]:\n",
    "    roll = shifted.rolling(w, min_periods=1)\n",
    "    grid[f\"rmean_{w}\"] = roll.mean()\n",
    "    grid[f\"rstd_{w}\"]  = roll.std().fillna(0)\n",
    "    if w in [7, 14, 28]:\n",
    "        grid[f\"rmax_{w}\"] = roll.max()\n",
    "        grid[f\"rmin_{w}\"] = roll.min()\n",
    "        grid[f\"rmed_{w}\"] = roll.median()\n",
    "\n",
    "for w in [7, 14, 28]:\n",
    "    grid[f\"rsum_{w}\"] = shifted.rolling(w, min_periods=1).sum()\n",
    "\n",
    "# Demand frequency rolling\n",
    "gh = grid.groupby(\"id_produit\")[\"has_demand\"]\n",
    "shifted_h = gh.shift(1)\n",
    "for w in [7, 14, 28, 60]:\n",
    "    grid[f\"dfreq_{w}\"] = shifted_h.rolling(w, min_periods=1).mean()\n",
    "\n",
    "# Days since last demand\n",
    "def _days_since(group):\n",
    "    had = group[\"has_demand\"].shift(1).values\n",
    "    n = len(group)\n",
    "    result = np.full(n, 999.0)\n",
    "    last = -9999\n",
    "    for i in range(n):\n",
    "        if i > 0 and had[i - 1] == 1:\n",
    "            last = i - 1\n",
    "        if last >= 0:\n",
    "            result[i] = float(i - last)\n",
    "    return pd.Series(result, index=group.index)\n",
    "\n",
    "grid[\"days_since\"] = grid.groupby(\"id_produit\", group_keys=False).apply(_days_since)\n",
    "\n",
    "# EWM & CV\n",
    "grid[\"cv_28\"] = grid[\"rstd_28\"] / (grid[\"rmean_28\"] + 1e-6)\n",
    "grid[\"ewm_7\"]  = g.shift(1).transform(lambda x: x.ewm(span=7, min_periods=1).mean())\n",
    "grid[\"ewm_28\"] = g.shift(1).transform(lambda x: x.ewm(span=28, min_periods=1).mean())\n",
    "\n",
    "# Calendar features + Fourier\n",
    "grid[\"dow\"] = grid[\"day\"].dt.dayofweek.astype(float)\n",
    "grid[\"month\"] = grid[\"day\"].dt.month.astype(float)\n",
    "grid[\"week\"] = grid[\"day\"].dt.isocalendar().week.astype(float)\n",
    "grid[\"is_wknd\"] = (grid[\"dow\"] >= 5).astype(float)\n",
    "grid[\"dom\"] = grid[\"day\"].dt.day.astype(float)\n",
    "grid[\"qtr\"] = grid[\"day\"].dt.quarter.astype(float)\n",
    "grid[\"day_idx\"] = (grid[\"day\"] - grid[\"day\"].min()).dt.days.astype(float)\n",
    "grid[\"day_idx_sq\"] = grid[\"day_idx\"] ** 2 / 1e6\n",
    "\n",
    "day_of_year = grid[\"day\"].dt.dayofyear.astype(float)\n",
    "for k in [1, 2, 3, 4]:\n",
    "    grid[f\"fourier_sin_y{k}\"] = np.sin(2 * np.pi * k * day_of_year / 365.25)\n",
    "    grid[f\"fourier_cos_y{k}\"] = np.cos(2 * np.pi * k * day_of_year / 365.25)\n",
    "for k in [1, 2]:\n",
    "    grid[f\"fourier_sin_w{k}\"] = np.sin(2 * np.pi * k * grid[\"dow\"] / 7)\n",
    "    grid[f\"fourier_cos_w{k}\"] = np.cos(2 * np.pi * k * grid[\"dow\"] / 7)\n",
    "for k in [1, 2]:\n",
    "    grid[f\"fourier_sin_m{k}\"] = np.sin(2 * np.pi * k * grid[\"dom\"] / 31)\n",
    "    grid[f\"fourier_cos_m{k}\"] = np.cos(2 * np.pi * k * grid[\"dom\"] / 31)\n",
    "\n",
    "grid[\"is_month_start\"] = (grid[\"dom\"] <= 3).astype(float)\n",
    "grid[\"is_month_end\"] = (grid[\"dom\"] >= 28).astype(float)\n",
    "grid[\"is_week_start\"] = (grid[\"dow\"] == 0).astype(float)\n",
    "\n",
    "print(f\"Features created. Grid shape: {grid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc93c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet-derived features\n",
    "pa = grid[\"prod_avg_demand\"] + 1e-6\n",
    "grid[\"prophet_ratio\"] = grid[\"prophet_yhat\"] / pa\n",
    "grid[\"prophet_over_ewm7\"] = grid[\"prophet_yhat\"] / (grid[\"ewm_7\"] + 1e-6)\n",
    "grid[\"prophet_over_rmean7\"] = grid[\"prophet_yhat\"] / (grid[\"rmean_7\"] + 1e-6)\n",
    "grid[\"prophet_residual\"] = grid[\"demand\"] - grid[\"prophet_yhat\"]\n",
    "grid[\"prophet_resid_lag1\"] = grid.groupby(\"id_produit\")[\"prophet_residual\"].shift(1)\n",
    "grid[\"prophet_resid_rmean7\"] = (grid.groupby(\"id_produit\")[\"prophet_residual\"]\n",
    "                                  .shift(1).rolling(7, min_periods=1).mean())\n",
    "grid[\"prophet_trend_norm\"] = grid[\"prophet_trend\"] / (pa + 1e-6)\n",
    "grid[\"prophet_weekly_abs\"] = grid[\"prophet_weekly\"].abs()\n",
    "grid[\"prophet_yearly_abs\"] = grid[\"prophet_yearly\"].abs()\n",
    "grid[\"prophet_seasonal_str\"] = grid[\"prophet_weekly_abs\"] + grid[\"prophet_yearly_abs\"]\n",
    "\n",
    "# Normalized features\n",
    "grid[\"lag1_norm\"] = grid[\"lag_1\"] / pa\n",
    "grid[\"rmean7_norm\"] = grid[\"rmean_7\"] / pa\n",
    "grid[\"rmean28_norm\"] = grid[\"rmean_28\"] / pa\n",
    "grid[\"ewm7_norm\"] = grid[\"ewm_7\"] / pa\n",
    "grid[\"ewm28_norm\"] = grid[\"ewm_28\"] / pa\n",
    "grid[\"rmean7_over_28\"] = grid[\"rmean_7\"] / (grid[\"rmean_28\"] + 1e-6)\n",
    "\n",
    "# Product static features\n",
    "pf = products[[\"id_produit\", \"total_demand\", \"demand_days\",\n",
    "               \"avg_demand\", \"demand_frequency\", \"priority_score\",\n",
    "               \"demand_score\", \"frequency_score\"]].copy()\n",
    "pf.columns = [\"id_produit\", \"p_total\", \"p_days\", \"p_avg\", \"p_freq\",\n",
    "               \"p_prio\", \"p_demand_score\", \"p_freq_score\"]\n",
    "grid = grid.merge(pf, on=\"id_produit\", how=\"left\")\n",
    "\n",
    "# Category encoding\n",
    "cat_map = products.set_index(\"id_produit\")[\"categorie\"]\n",
    "grid[\"categorie\"] = grid[\"id_produit\"].map(cat_map).fillna(\"UNKNOWN\")\n",
    "grid[\"cat_enc\"] = grid[\"categorie\"].astype(\"category\").cat.codes.astype(float)\n",
    "\n",
    "# Segment x Temporal interactions\n",
    "grid[\"hf_x_dow\"] = grid[\"is_hf\"] * grid[\"dow\"]\n",
    "grid[\"hf_x_month\"] = grid[\"is_hf\"] * grid[\"month\"]\n",
    "grid[\"hf_x_is_wknd\"] = grid[\"is_hf\"] * grid[\"is_wknd\"]\n",
    "grid[\"hf_x_lag1\"] = grid[\"is_hf\"] * grid[\"lag_1\"].fillna(0)\n",
    "grid[\"hf_x_rmean7\"] = grid[\"is_hf\"] * grid[\"rmean_7\"].fillna(0)\n",
    "grid[\"hf_x_prophet\"] = grid[\"is_hf\"] * grid[\"prophet_yhat\"]\n",
    "\n",
    "# Drop warm-up (60 days)\n",
    "grid = grid[grid[\"day\"] >= grid[\"day\"].min() + pd.Timedelta(days=60)].copy()\n",
    "grid.drop(columns=[\"prophet_residual\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Clean infinities\n",
    "for c in grid.select_dtypes(include=[np.number]).columns:\n",
    "    grid[c] = grid[c].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "print(f\"Final grid: {grid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f885bdc",
   "metadata": {},
   "source": [
    "## 7. Train/Test Split & XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c2736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature list and temporal split\n",
    "EXCLUDE_COLS = {\"day\", \"id_produit\", \"demand\", \"has_demand\", \"categorie\",\n",
    "                \"segment\", \"n_deliveries\", \"qty_delivered\"}\n",
    "FEATURE_COLS = sorted([c for c in grid.columns if c not in EXCLUDE_COLS])\n",
    "print(f\"Features: {len(FEATURE_COLS)}\")\n",
    "\n",
    "# Temporal split: last 30 days = test\n",
    "split_date = grid[\"day\"].max() - pd.Timedelta(days=30)\n",
    "train_full = grid[grid[\"day\"] <= split_date].copy()\n",
    "test_full  = grid[grid[\"day\"] >  split_date].copy()\n",
    "\n",
    "train_full[FEATURE_COLS] = train_full[FEATURE_COLS].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "test_full[FEATURE_COLS]  = test_full[FEATURE_COLS].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "train_pos = train_full[train_full[\"has_demand\"] == 1].copy()\n",
    "test_pos  = test_full[test_full[\"has_demand\"] == 1].copy()\n",
    "\n",
    "print(f\"Train: {len(train_full):,} total, {len(train_pos):,} demand rows\")\n",
    "print(f\"Test:  {len(test_full):,} total, {len(test_pos):,} demand rows\")\n",
    "print(f\"Split date: {split_date.date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c92dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost binary classifier (demand yes/no)\n",
    "y_tr_cls = train_full[\"has_demand\"]\n",
    "y_te_cls = test_full[\"has_demand\"]\n",
    "pos = y_tr_cls.sum()\n",
    "neg = len(y_tr_cls) - pos\n",
    "\n",
    "cls_params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"max_depth\": 7,\n",
    "    \"learning_rate\": 0.04,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.6,\n",
    "    \"colsample_bylevel\": 0.8,\n",
    "    \"min_child_weight\": 15,\n",
    "    \"scale_pos_weight\": neg / max(pos, 1),\n",
    "    \"reg_alpha\": 0.5,\n",
    "    \"reg_lambda\": 3.0,\n",
    "    \"gamma\": 0.1,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "d_tr_cls = xgb.DMatrix(train_full[FEATURE_COLS], label=y_tr_cls)\n",
    "d_te_cls = xgb.DMatrix(test_full[FEATURE_COLS],  label=y_te_cls)\n",
    "\n",
    "t0 = time.time()\n",
    "classifier = xgb.train(\n",
    "    cls_params, d_tr_cls, num_boost_round=800,\n",
    "    evals=[(d_te_cls, \"val\")], early_stopping_rounds=50,\n",
    "    verbose_eval=100,\n",
    ")\n",
    "print(f\"\\nTraining time: {time.time()-t0:.0f}s\")\n",
    "print(f\"Best iteration: {classifier.best_iteration}\")\n",
    "\n",
    "proba_te = classifier.predict(d_te_cls)\n",
    "auc = roc_auc_score(y_te_cls, proba_te)\n",
    "print(f\"Classifier AUC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b71d68e",
   "metadata": {},
   "source": [
    "## 8. Threshold, Blend & Bias Tuning\n",
    "\n",
    "3D grid search over:\n",
    "- **Threshold**: classifier probability cutoff for demand prediction\n",
    "- **Alpha**: blend weight (Prophet vs EWM-7)\n",
    "- **Bias multiplier**: global scaling factor\n",
    "\n",
    "Optimization target: minimize WAPE + bias penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9439e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation setup\n",
    "test_eval = test_full.reset_index(drop=True).copy()\n",
    "d_te_all = xgb.DMatrix(test_eval[FEATURE_COLS])\n",
    "proba_all = classifier.predict(d_te_all)\n",
    "is_hf_test = test_eval[\"is_hf\"].values\n",
    "actual_all = test_eval[\"demand\"].values\n",
    "has_demand_mask = actual_all > 0\n",
    "a_dd_sum = actual_all[has_demand_mask].sum()\n",
    "\n",
    "prophet_base = test_eval[\"prophet_yhat\"].fillna(0).values\n",
    "ewm_base = test_eval[\"ewm_7\"].fillna(0).values\n",
    "\n",
    "def evaluate_fast(thr, alpha, bias_mult):\n",
    "    pred_mask = proba_all >= thr\n",
    "    pred_demand = np.zeros(len(test_eval))\n",
    "    if pred_mask.sum() > 0:\n",
    "        blended = alpha * prophet_base[pred_mask] + (1 - alpha) * ewm_base[pred_mask]\n",
    "        pred_demand[pred_mask] = blended * bias_mult\n",
    "    p_dd = pred_demand[has_demand_mask]\n",
    "    a_dd = actual_all[has_demand_mask]\n",
    "    if a_dd_sum == 0:\n",
    "        return 999, 0\n",
    "    wape = np.sum(np.abs(a_dd - p_dd)) / a_dd_sum * 100\n",
    "    bias = (p_dd.sum() - a_dd_sum) / a_dd_sum * 100\n",
    "    return wape, bias\n",
    "\n",
    "# Coarse grid search\n",
    "print(\"Coarse grid search...\")\n",
    "best_score = 1e9\n",
    "best_thr, best_alpha, best_bm = 0.1, 0.7, 1.0\n",
    "\n",
    "for thr in np.arange(0.02, 0.50, 0.02):\n",
    "    for alpha in np.arange(0.0, 1.05, 0.1):\n",
    "        for bm in np.arange(0.5, 2.0, 0.1):\n",
    "            w, b = evaluate_fast(thr, alpha, bm)\n",
    "            bias_pen = abs(b) * 3 if b < 0 else (max(0, b - 5) * 3)\n",
    "            score = w + bias_pen\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_thr, best_alpha, best_bm = thr, alpha, bm\n",
    "\n",
    "print(f\"Coarse best: thr={best_thr:.2f}, alpha={best_alpha:.2f}, bm={best_bm:.2f}\")\n",
    "\n",
    "# Fine grid search\n",
    "print(\"Fine grid search...\")\n",
    "for thr in np.arange(max(0.02, best_thr - 0.04), min(0.50, best_thr + 0.04), 0.005):\n",
    "    for alpha in np.arange(max(0.0, best_alpha - 0.15), min(1.05, best_alpha + 0.15), 0.025):\n",
    "        for bm in np.arange(max(0.5, best_bm - 0.2), min(2.0, best_bm + 0.2), 0.01):\n",
    "            w, b = evaluate_fast(thr, alpha, bm)\n",
    "            bias_pen = abs(b) * 3 if b < 0 else (max(0, b - 5) * 3)\n",
    "            score = w + bias_pen\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_thr, best_alpha, best_bm = thr, alpha, bm\n",
    "\n",
    "w_final, b_final = evaluate_fast(best_thr, best_alpha, best_bm)\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"BEST PARAMETERS:\")\n",
    "print(f\"  Threshold:    {best_thr:.3f}\")\n",
    "print(f\"  Alpha:        {best_alpha:.3f}\")\n",
    "print(f\"  Bias mult:    {best_bm:.3f}\")\n",
    "print(f\"  WAPE (dd):    {w_final:.2f}%\")\n",
    "print(f\"  Bias (dd):    {b_final:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f5ae9",
   "metadata": {},
   "source": [
    "## 9. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbdd27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full evaluation with best parameters\n",
    "pred_mask = proba_all >= best_thr\n",
    "pred_final = np.zeros(len(test_eval))\n",
    "if pred_mask.sum() > 0:\n",
    "    blended = best_alpha * prophet_base[pred_mask] + (1 - best_alpha) * ewm_base[pred_mask]\n",
    "    pred_final[pred_mask] = blended * best_bm\n",
    "\n",
    "actual_full = test_eval[\"demand\"].values\n",
    "has_demand = actual_full > 0\n",
    "\n",
    "# Demand-days metrics\n",
    "a_dd = actual_full[has_demand]\n",
    "p_dd = pred_final[has_demand]\n",
    "wape_dd = np.sum(np.abs(a_dd - p_dd)) / a_dd.sum() * 100\n",
    "bias_dd = (p_dd.sum() - a_dd.sum()) / a_dd.sum() * 100\n",
    "\n",
    "# Full-grid metrics\n",
    "wape_grid = np.sum(np.abs(actual_full - pred_final)) / actual_full.sum() * 100\n",
    "bias_grid = (pred_final.sum() - actual_full.sum()) / actual_full.sum() * 100\n",
    "\n",
    "# Baselines\n",
    "bl_lag1 = test_eval[\"lag_1\"].fillna(0).values\n",
    "bl_lag1_dd = np.sum(np.abs(a_dd - bl_lag1[has_demand])) / a_dd.sum() * 100\n",
    "bl_ewm = test_eval[\"ewm_7\"].fillna(0).values\n",
    "bl_ewm_dd = np.sum(np.abs(a_dd - bl_ewm[has_demand])) / a_dd.sum() * 100\n",
    "bl_prophet_dd = np.sum(np.abs(a_dd - prophet_base[has_demand])) / a_dd.sum() * 100\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Demand-Days  WAPE : {wape_dd:.2f}%  (target < 50%)\")\n",
    "print(f\"  Demand-Days  Bias : {bias_dd:.2f}%  (target 0-5%)\")\n",
    "print(f\"  Full-Grid    WAPE : {wape_grid:.1f}%\")\n",
    "print(f\"  Full-Grid    Bias : {bias_grid:.1f}%\")\n",
    "print(f\"\")\n",
    "print(f\"  Baseline Lag-1 (dd):   {bl_lag1_dd:.1f}%\")\n",
    "print(f\"  Baseline EWM-7 (dd):   {bl_ewm_dd:.1f}%\")\n",
    "print(f\"  Baseline Prophet (dd): {bl_prophet_dd:.1f}%\")\n",
    "print(f\"\")\n",
    "improv = (bl_lag1_dd - wape_dd) / bl_lag1_dd * 100\n",
    "print(f\"  Improvement vs Lag-1:  {improv:.1f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Per-segment evaluation\n",
    "for seg_name, seg_val in [(\"HF\", 1.0), (\"LF\", 0.0)]:\n",
    "    seg_mask = has_demand & (is_hf_test == seg_val)\n",
    "    if seg_mask.sum() > 0:\n",
    "        a_s = actual_full[seg_mask]\n",
    "        p_s = pred_final[seg_mask]\n",
    "        sw = np.sum(np.abs(a_s - p_s)) / a_s.sum() * 100\n",
    "        sb = (p_s.sum() - a_s.sum()) / a_s.sum() * 100\n",
    "        print(f\"  {seg_name} demand-days: WAPE={sw:.1f}%, Bias={sb:.1f}%, n={seg_mask.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d78d5b",
   "metadata": {},
   "source": [
    "## 10. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11092b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save XGBoost classifier\n",
    "classifier.save_model(str(MODEL / \"xgboost_classifier_model.json\"))\n",
    "\n",
    "# Train and save minimal regressor (for API compatibility)\n",
    "y_tr_reg = np.log1p(train_pos[\"demand\"].values)\n",
    "y_te_reg = np.log1p(test_pos[\"demand\"].values)\n",
    "reg_params = {\n",
    "    \"objective\": \"reg:squarederror\", \"eval_metric\": \"mae\",\n",
    "    \"max_depth\": 4, \"learning_rate\": 0.05, \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.6, \"min_child_weight\": 20,\n",
    "    \"reg_alpha\": 1.0, \"reg_lambda\": 5.0, \"tree_method\": \"hist\", \"seed\": 42,\n",
    "}\n",
    "d_tr_reg = xgb.DMatrix(train_pos[FEATURE_COLS], label=y_tr_reg)\n",
    "d_te_reg = xgb.DMatrix(test_pos[FEATURE_COLS],  label=y_te_reg)\n",
    "regressor = xgb.train(\n",
    "    reg_params, d_tr_reg, num_boost_round=500,\n",
    "    evals=[(d_te_reg, \"val\")], early_stopping_rounds=50, verbose_eval=0,\n",
    ")\n",
    "regressor.save_model(str(MODEL / \"xgboost_regression_model.json\"))\n",
    "\n",
    "# Save Prophet metadata\n",
    "with open(MODEL / \"prophet_meta.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"prophet_models\": {str(k): v for k, v in prophet_models_meta.items()},\n",
    "        \"simple_avg\": {str(k): v for k, v in simple_avg.items()},\n",
    "        \"segment_map\": {str(k): v for k, v in seg_map.items()},\n",
    "    }, f, indent=2)\n",
    "\n",
    "# Save product attributes\n",
    "prod_attrs = products_raw.set_index(\"id_produit\").to_dict(\"index\")\n",
    "with open(MODEL / \"product_attributes.json\", \"w\") as f:\n",
    "    json.dump({str(k): v for k, v in prod_attrs.items()}, f, indent=2)\n",
    "\n",
    "# Save delivery stats\n",
    "del_stats_dict = prod_del_stats.set_index(\"id_produit\").to_dict(\"index\")\n",
    "with open(MODEL / \"delivery_stats.json\", \"w\") as f:\n",
    "    json.dump({str(k): v for k, v in del_stats_dict.items()}, f, indent=2)\n",
    "\n",
    "# Save category encoding\n",
    "cat_enc_map = dict(zip(\n",
    "    grid[\"categorie\"].astype(\"category\").cat.categories,\n",
    "    range(len(grid[\"categorie\"].astype(\"category\").cat.categories))\n",
    "))\n",
    "with open(MODEL / \"cat_encoding.json\", \"w\") as f:\n",
    "    json.dump(cat_enc_map, f, indent=2)\n",
    "\n",
    "# Save config\n",
    "config = {\n",
    "    \"optimal_threshold\": round(float(best_thr), 4),\n",
    "    \"bias_multiplier\": round(float(best_bm), 4),\n",
    "    \"ensemble_alpha\": round(float(best_alpha), 4),\n",
    "    \"feature_cols_regression\": FEATURE_COLS,\n",
    "    \"model_type\": \"prophet_classifier_blend_v9\",\n",
    "    \"performance\": {\n",
    "        \"wape_demand_days\": round(float(wape_dd), 2),\n",
    "        \"bias_demand_days\": round(float(bias_dd), 2),\n",
    "        \"wape_full_grid\": round(float(wape_grid), 2),\n",
    "        \"bias_full_grid\": round(float(bias_grid), 2),\n",
    "        \"baseline_lag1_wape_dd\": round(float(bl_lag1_dd), 2),\n",
    "        \"baseline_ewm7_wape_dd\": round(float(bl_ewm_dd), 2),\n",
    "        \"baseline_prophet_wape_dd\": round(float(bl_prophet_dd), 2),\n",
    "        \"improvement_vs_lag1_pct\": round(float(improv), 1),\n",
    "        \"classifier_auc\": round(float(auc), 4),\n",
    "    },\n",
    "}\n",
    "with open(MODEL / \"forecast_config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"All models saved to:\", MODEL.resolve())\n",
    "print(\"Files saved:\")\n",
    "for f in MODEL.glob(\"*.json\"):\n",
    "    print(f\"  {f.name}: {f.stat().st_size / 1024:.0f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9049e34",
   "metadata": {},
   "source": [
    "## 11. Task 1: Warehouse Optimization Approach\n",
    "\n",
    "Task 1 uses a **heuristic/algorithmic approach** (no ML model needed):\n",
    "\n",
    "### Storage Assignment Strategy\n",
    "1. **Segment-aware placement**: HF products -> PICKING zone (close to expedition), LF products -> RESERVE zone\n",
    "2. **Scoring function**: `score = -dist_to_expedition * w1 - floor * w2` (HF gets higher distance weight)\n",
    "3. **Heavy item handling**: Extra ground floor penalty for products > 5kg\n",
    "4. **State tracking**: In-memory slot occupancy prevents double-assignment\n",
    "\n",
    "### Picking Optimization Strategy\n",
    "1. **Multi-chariot splitting**: Items split across chariots by weight capacity (300kg max)\n",
    "2. **Nearest-neighbor routing**: Each chariot follows nearest-unvisited-location path\n",
    "3. **Congestion detection**: Zone visit counting warns when >3 picks in same zone\n",
    "4. **Distance calculation**: Manhattan 3D distance (x, y, z) between locations\n",
    "\n",
    "### Simulation Engine\n",
    "- Processes events chronologically\n",
    "- Ingoing: assigns optimal storage with demand forecasting context\n",
    "- Outgoing: picks from stored location, generates route, releases slot\n",
    "- Tracks warehouse state throughout\n",
    "\n",
    "Implementation: See `inference_optimization.py` and `main.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8958279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate warehouse optimization with sample data\n",
    "warehouse_locations = pd.read_csv(DATA / \"warehouse_locations.csv\")\n",
    "\n",
    "print(\"=== Warehouse Layout ===\")\n",
    "print(f\"Total locations: {len(warehouse_locations)}\")\n",
    "print(f\"\\nBy type:\")\n",
    "print(warehouse_locations['type_emplacement'].value_counts())\n",
    "print(f\"\\nBy zone:\")\n",
    "print(warehouse_locations['zone'].value_counts())\n",
    "print(f\"\\nFloor distribution:\")\n",
    "print(warehouse_locations['z'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47453d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate storage assignment logic\n",
    "print(\"=== Storage Assignment Demo ===\")\n",
    "print(\"\")\n",
    "\n",
    "# HF Product example\n",
    "hf_product = 31554\n",
    "hf_info = products[products['id_produit'] == hf_product].iloc[0]\n",
    "seg = segments[segments['id_produit'] == hf_product]['segment'].values[0]\n",
    "print(f\"Product {hf_product} (Segment: {seg}):\")\n",
    "print(f\"  Demand frequency: {hf_info['demand_frequency']:.4f}\")\n",
    "print(f\"  Avg demand: {hf_info['avg_demand']:.0f}\")\n",
    "print(f\"  -> PLACEMENT: PICKING zone (close to expedition)\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# LF Product example\n",
    "lf_product = 31339\n",
    "lf_info = products[products['id_produit'] == lf_product].iloc[0]\n",
    "seg = segments[segments['id_produit'] == lf_product]['segment'].values[0]\n",
    "print(f\"Product {lf_product} (Segment: {seg}):\")\n",
    "print(f\"  Demand frequency: {lf_info['demand_frequency']:.4f}\")\n",
    "print(f\"  Avg demand: {lf_info['avg_demand']:.0f}\")\n",
    "print(f\"  -> PLACEMENT: RESERVE zone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b9c1a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Model Files Produced\n",
    "| File | Size | Description |\n",
    "|------|------|-------------|\n",
    "| `xgboost_classifier_model.json` | ~2 MB | Binary demand classifier |\n",
    "| `xgboost_regression_model.json` | ~1 MB | Regressor (API compat.) |\n",
    "| `prophet_meta.json` | ~200 KB | Per-SKU Prophet params (571 products) |\n",
    "| `forecast_config.json` | ~10 KB | Threshold, blend, features |\n",
    "| `product_attributes.json` | ~100 KB | Physical product attributes |\n",
    "| `delivery_stats.json` | ~50 KB | Delivery pattern stats |\n",
    "| `cat_encoding.json` | ~1 KB | Category encoding map |\n",
    "\n",
    "### All Models < 5 MB total (lightweight)\n",
    "\n",
    "### Inference Scripts\n",
    "- `inference_forecast.py`: Task 2 standalone inference\n",
    "- `inference_optimization.py`: Task 1 standalone inference\n",
    "- `main.py`: Full API with all endpoints"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
